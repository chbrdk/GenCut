# Video Analysis Agent – Function Descriptions

Below is a separate, detailed description for every endpoint (function) provided by the Video Analysis Agent. Each description explains what the function does, what it requires (input), and what it returns (output). (No emoticons.)

──────────────────────────────────────────────────────────────────────────────
Function: POST /separate (Analyzer Service)
──────────────────────────────────────────────────────────────────────────────
Purpose:
  Separates an uploaded video file into two separate streams – a video-only stream (without audio) and an audio-only stream (extracted as an MP3). This separation is intended to allow parallel processing (for example, transcribing the audio and analyzing the video scenes simultaneously).

Input:
  • A multipart form (Content-Type: multipart/form-data) containing a video file (e.g. MP4, AVI, etc.) under the key "file".
  
Output (JSON):
  {
    "video_id": "uuid-string", (a unique identifier for the uploaded video)
    "filename": "original_filename.mp4", (the original filename of the uploaded video)
    "video_url": "/separated/filename_video.mp4", (a relative URL (or path) to the separated video-only file)
    "audio_url": "/separated/filename_audio.mp3" (a relative URL (or path) to the separated audio-only file (MP3))
  }
  
Notes:
  • Always call this endpoint first (i.e. "Step 1") so that you can later process the video and audio in parallel.
  • The "video_url" and "audio_url" (or "video_path" and "audio_path") are relative (or absolute) paths (or URLs) that you can use in subsequent calls (for example, to /asr or /analyze).

──────────────────────────────────────────────────────────────────────────────
Function: POST /asr (Whisper Service)
──────────────────────────────────────────────────────────────────────────────
Purpose:
  Transcribes an audio file (for example, an MP3 or WAV file) into plain text using the OpenAI Whisper model. This endpoint is intended for speech-to-text conversion.

Input:
  • A multipart form (Content-Type: multipart/form-data) containing an audio file (e.g. MP3, WAV) under the key "audio_file".
  
Output (JSON):
  {
    "text": "Complete transcribed text content from the audio file"
  }
  
Notes:
  • Typically, you'd call this endpoint (or "Step 2A") with the audio file (or "audio_url") obtained from the /separate endpoint.
  • The "text" field contains the full transcription (plain text) of the audio.

──────────────────────────────────────────────────────────────────────────────
Function: POST /analyze (Analyzer Service)
──────────────────────────────────────────────────────────────────────────────
Purpose:
  Performs a comprehensive analysis on a video file. It detects scene boundaries (using PySceneDetect) and then, for each scene (or for representative frames), it runs an AI-powered visual analysis (using BLIP for scene description and YOLO for object detection). The output includes scene metadata (start/end times, screenshots) and, for each screenshot, a detailed "analysis" (description, detected objects, scene category, action, and an "importance" score).

Input:
  • A multipart form (Content-Type: multipart/form-data) containing a video file (e.g. MP4) under the key "file".
  
Output (JSON):
  {
    "video_id": "uuid-string", (a unique identifier for the uploaded video)
    "filename": "video.mp4", (the original filename of the uploaded video)
    "scenes": [
      {
        "scene": 0, (scene index (integer))
        "start_time": "00:00:00.000", (scene start time (string, timecode))
        "end_time": "00:00:05.123", (scene end time (string, timecode))
        "screenshots": [
          {
            "url": "/videos/screenshots/video/scene_000_frame_000.jpg", (a relative URL (or path) to the screenshot image)
            "timestamp": "0:00:01.500000", (the timestamp (string) of the frame (e.g. "0:00:01.500000"))
            "frame_number": 45, (the frame number (integer) in the video)
            "analysis": {
              "description": "Natural language description of the scene (generated by BLIP).",
              "objects": [
                {
                  "class": "person|car|dog|etc", (the detected object's class (string))
                  "confidence": 0.95, (a confidence score (float) (e.g. 0.95))
                  "position": [x1, y1, x2, y2] (a list of four floats (or integers) representing the bounding box (x1, y1, x2, y2) of the object (e.g. [100, 50, 200, 300]))
                }
              ],
              "category": "action|dialogue|landscape|group scene|close-up|general", (a string (or enum) indicating the scene's category (e.g. "action", "dialogue", "landscape", "group scene", "close-up", "general"))
              "action": "walking|running|talking|fighting|driving|sitting|standing|unknown", (a string (or enum) indicating the detected action (e.g. "walking", "running", "talking", "fighting", "driving", "sitting", "standing", "unknown"))
              "importance_score": 0.75 (a float (or integer) (e.g. 0.75) indicating the "importance" (or relevance) of the scene (or frame) (for example, for content curation))
            }
          }
        ]
      }
    ]
  }
  
Notes:
  • Typically, you'd call this endpoint (or "Step 2B") with the video file (or "video_url") obtained from the /separate endpoint.
  • The "scenes" array contains one (or more) scene objects. Each scene object contains "scene" (index), "start_time", "end_time", and an array "screenshots" (one (or more) screenshot objects). Each screenshot object contains "url", "timestamp", "frame_number", and an "analysis" object (with "description", "objects", "category", "action", and "importance_score").
  • "objects" is an array of detected objects (each with "class", "confidence", and "position" (a bounding box)).
  • "category" and "action" are strings (or enums) (for example, "action", "dialogue", "landscape", "group scene", "close-up", "general" (for "category") and "walking", "running", "talking", "fighting", "driving", "sitting", "standing", "unknown" (for "action")).
  • "importance_score" is a float (or integer) (for example, 0.75) indicating the "importance" (or relevance) of the scene (or frame) (for example, for content curation).

──────────────────────────────────────────────────────────────────────────────
Function: POST /analyze-screenshot (Analyzer Service)
──────────────────────────────────────────────────────────────────────────────
Purpose:
  Analyzes a single image (or "screenshot") (for example, a JPG or PNG file) using the AI models (BLIP for scene description and YOLO for object detection). This endpoint is intended for "on-demand" (or "one-shot") image analysis.

Input:
  • A multipart form (Content-Type: multipart/form-data) containing an image file (e.g. JPG, PNG) under the key "file".
  
Output (JSON):
  {
    "screenshot_id": "uuid-string", (a unique identifier (or "uuid") for the uploaded image)
    "filename": "image.jpg", (the original filename of the uploaded image)
    "analysis": {
      "description": "Natural language description (generated by BLIP).",
      "objects": [ (an array of detected objects (each with "class", "confidence", and "position" (a bounding box)) (e.g. [{"class": "person", "confidence": 0.95, "position": [100, 50, 200, 300]}])
      ],
      "category": "scene category (e.g. "action", "dialogue", "landscape", "group scene", "close-up", "general")",
      "action": "detected action (e.g. "walking", "running", "talking", "fighting", "driving", "sitting", "standing", "unknown")",
      "importance_score": 0.75 (a float (or integer) (e.g. 0.75) indicating the "importance" (or relevance) of the image (for example, for content curation))
    }
  }
  
Notes:
  • This endpoint is intended for "on-demand" (or "one-shot") image analysis (for example, if you upload a single screenshot (or "image") and want its AI analysis).
  • "objects" is an array of detected objects (each with "class", "confidence", and "position" (a bounding box)).
  • "category" and "action" are strings (or enums) (for example, "action", "dialogue", "landscape", "group scene", "close-up", "general" (for "category") and "walking", "running", "talking", "fighting", "driving", "sitting", "standing", "unknown" (for "action")).
  • "importance_score" is a float (or integer) (for example, 0.75) indicating the "importance" (or relevance) of the image (for example, for content curation).

──────────────────────────────────────────────────────────────────────────────
Function: POST /cutdown (Analyzer Service)
──────────────────────────────────────────────────────────────────────────────
Purpose:
  Extracts (or "cuts") a segment (or "clip") from a video (for example, a video file (or "video_id")) between two timestamps (for example, "start" and "end"). This endpoint is intended for "trimming" (or "cutting") a video (for example, for "cutdowns" or "clips").

Input:
  • A JSON payload (Content-Type: application/json) with the following keys:
  – "video_id" (string) (for example, "filename.mp4" (or "uuid")) (the identifier (or "filename") of the video (or "video file") to "cut" (or "trim")).
  – "start" (string) (for example, "00:00:10") (the "start" timestamp (or "time") (in "HH:MM:SS" (or "HH:MM:SS.mmm") format) (for example, "00:00:10" (or "00:00:10.000"))).
  – "end" (string) (for example, "00:00:20") (the "end" timestamp (or "time") (in "HH:MM:SS" (or "HH:MM:SS.mmm") format) (for example, "00:00:20" (or "00:00:20.000"))).
  
Output (JSON):
  {
    "success": true, (a boolean (or "flag") (for example, true) indicating whether the "cut" (or "trim") was successful (or "true") (or "false" (or "error").))
    "output": "videos/cutdowns/video_id_cut.mp4" (a string (or "path") (or "url") (for example, "videos/cutdowns/filename_cut.mp4") (the (relative (or absolute)) path (or "url") (or "filename") of the "cut" (or "trimmed") video (or "clip".)
  }
  
Notes:
  • This endpoint is intended for "trimming" (or "cutting") a video (for example, for "cutdowns" (or "clips")).
  • "video_id" (or "filename") (for example, "filename.mp4" (or "uuid")) is the identifier (or "filename") of the video (or "video file") to "cut" (or "trim").
  • "start" (or "end") (for example, "00:00:10" (or "00:00:20")) is the "start" (or "end") timestamp (or "time") (in "HH:MM:SS" (or "HH:MM:SS.mmm") format) (for example, "00:00:10" (or "00:00:10.000") (or "00:00:20" (or "00:00:20.000"))).
  • "output" (or "path") (or "url") (for example, "videos/cutdowns/filename_cut.mp4") is the (relative (or absolute)) path (or "url") (or "filename") of the "cut" (or "trimmed") video (or "clip".
  • "success" (or "flag") (for example, true) is a boolean (or "flag") (for example, true) indicating whether the "cut" (or "trim") was successful (or "true") (or "false" (or "error")).

──────────────────────────────────────────────────────────────────────────────
Summary (or "Recommended Workflow" (or "Integration Workflow")):
──────────────────────────────────────────────────────────────────────────────
• Step 1 (or "Separate"): Call POST /separate (Analyzer Service) (with the original video file) → obtain "video_url" (or "video_path") and "audio_url" (or "audio_path").
• Step 2 (or "Parallel Processing"):
  – Branch A (or "Audio"): Call POST /asr (Whisper Service) (with "audio_url" (or "audio_path")) → obtain "text" (or "transcription").
  – Branch B (or "Video"): Call POST /analyze (Analyzer Service) (with "video_url" (or "video_path")) → obtain "scenes" (or "scene analysis" (or "visual analysis")).
• Step 3 (or "Merge" (or "Combine")): Merge (or "combine") the "text" (or "transcription") (from "Branch A") with "scenes" (or "scene analysis" (or "visual analysis")) (from "Branch B") (for example, by mapping "transcription" timestamps (or "text") to "scene" timestamps (or "scenes") (or "screenshots")) → obtain a "unified" (or "comprehensive") "video analysis" (or "video understanding").
• (Optional) (or "On-Demand" (or "One-Shot")): Call POST /analyze-screenshot (Analyzer Service) (with a single image (or "screenshot") (for example, a JPG (or PNG) file)) → obtain "analysis" (or "image analysis" (or "screenshot analysis")).
• (Optional) (or "Cut" (or "Trim")): Call POST /cutdown (Analyzer Service) (with "video_id" (or "filename"), "start" (or "start time"), and "end" (or "end time")) → obtain "output" (or "cut" (or "trimmed") "clip" (or "video")).

──────────────────────────────────────────────────────────────────────────────
End of Function Descriptions.
────────────────────────────────────────────────────────────────────────────── 